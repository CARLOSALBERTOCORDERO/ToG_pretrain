{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26KJjVGTRAzr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=aQaZMC9-Jok\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential  # Can not be moved\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D  # Can not be moved\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense # Can not be moved\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('train20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='tog-dataset-sagemaker' # Replace with your s3 bucket name\n",
    "Filename = \"dataset.zip\"\n",
    "s3 = boto3.Session().resource('s3').Bucket(bucket).Object(Filename).download_file('dataset.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image_RGB):\n",
    "    image_0_2_1 = image_RGB\n",
    "    print(image_0_2_1.shape)\n",
    "    plt.imshow(image_0_2_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data/data/train/cat/cat.3.jpg\"\n",
    "img = load_img(img_path, target_size=(100, 100))\n",
    "\n",
    "imgArray = img_to_array(img)\n",
    "# Update to 0-1 format\n",
    "imgArray = imgArray / 255.0\n",
    "print(imgArray.shape)\n",
    "plt.imshow(imgArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"data/data/train\"\n",
    "test_data = \"data/data/test\"\n",
    "\n",
    "# Número de iteraciones sobre todo el dataset de entrenamiento\n",
    "epochs = 20\n",
    "\n",
    "# Dimensiones de las imágenes para procesar\n",
    "n_H, n_W = 100, 100\n",
    "\n",
    "# Utilizaremos mini-batch\n",
    "batch_size = 32\n",
    "\n",
    "# Número de iteraciones que vamos a procesar la información en cada epoca (entrenamiento)\n",
    "train_samples = 1000\n",
    "\n",
    "# Número de iteraciones que vamos a procesar la información en cada epoca (validación)\n",
    "validation_samples = 200\n",
    "\n",
    "# Definamos la tasa de aprendizaje\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Número de clases\n",
    "class_num = 3\n",
    "\n",
    "# Estructura de la red neuronal convolucionales\n",
    "filter_conv1 = 32\n",
    "size_filter1 = (3,3)\n",
    "\n",
    "filter_conv2 = 64\n",
    "size_filter2 = (2,2)\n",
    "\n",
    "#Usaremos un Max Pooling\n",
    "size_pool = (2,2)\n",
    "\n",
    "\n",
    "# 1. Antes de comenzar con el modelo, vamos a preprocesar las imágenes\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,          # Normalizar los valores de los pixeles\n",
    "    shear_range= 0.3,           # Rango del ángulo que podemos inclinar nuestras imágenes\n",
    "    zoom_range = 0.3,            # Rango del zoom que podemos hacer a nuestras imágenes\n",
    "    horizontal_flip = True       # Invierte imágenes\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255           # Normalizar los valores de los pixeles\n",
    ")\n",
    "\n",
    "# Accede al directorio, preprocesa las imágenes y organiza en mini-batchs\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size = (n_H, n_W),             # Tamaño de las imágenes\n",
    "    batch_size = batch_size,              # Tamaño del mini-batch\n",
    "    class_mode = 'categorical'            # Modelo para clasificación\n",
    ")\n",
    "\n",
    "# Accede al directorio, preprocesa las imágenes y organiza en mini-batchs\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    test_data,\n",
    "    target_size = (n_H, n_W),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for TensorFlow or Thieno\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    input_shape = (3, n_W, n_H)\n",
    "else:\n",
    "    input_shape = (n_W, n_H, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear la ConvNet\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filter_conv1, size_filter1, padding=\"same\", input_shape=(n_H, n_W,3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=size_pool))\n",
    "    model.add(Conv2D(filter_conv2, size_filter2, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=size_pool))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(255, activation=None))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(class_num, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.class_indices)\n",
    "imgs, labels = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = train_generator.next()\n",
    "\n",
    "print(len(image_batch))\n",
    "for index in range(0, len(image_batch)):\n",
    "    image = image_batch[index]\n",
    "    print(label_batch[index])\n",
    "    imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel()\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.Adam(learning_rate),  \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=train_samples, \n",
    "    epochs=epochs, \n",
    "    validation_data = validation_generator, \n",
    "    validation_steps=validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")\n",
    "model.save_weights(\"weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='sn2020-final-project' # Replace with your s3 bucket name\n",
    "keymodel = 'model.h5'\n",
    "keyweights = \"weights.h5\"\n",
    "\n",
    "url = 's3://{}/{}'.format(bucket, keymodel)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(keymodel).upload_file('model.h5')\n",
    "print('Done writing to {}'.format(url))\n",
    "\n",
    "url = 's3://{}/{}'.format(bucket, keyweights)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(keyweights).upload_file('weights.h5')\n",
    "print('Done writing to {}'.format(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'utils.ipynp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e909faf621b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3://{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done writing to {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mobject_upload_file\u001b[0;34m(self, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    278\u001b[0m     return self.meta.client.upload_file(\n\u001b[1;32m    279\u001b[0m         \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    129\u001b[0m         return transfer.upload_file(\n\u001b[1;32m    130\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    277\u001b[0m             filename, bucket, key, extra_args, subscribers)\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;31m# If a client error was raised, add the backwards compatibility layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# that raises a S3UploadFailedError. These specific errors were only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Determine the size if it was not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mupload_input_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide_transfer_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# Do a multipart upload if needed, otherwise do a regular put object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mprovide_transfer_size\u001b[0;34m(self, transfer_future)\u001b[0m\n\u001b[1;32m    235\u001b[0m         transfer_future.meta.provide_transfer_size(\n\u001b[1;32m    236\u001b[0m             self._osutil.get_file_size(\n\u001b[0;32m--> 237\u001b[0;31m                 transfer_future.meta.call_args.fileobj))\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires_multipart_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mget_file_size\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_file_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_chunk_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_byte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'utils.ipynp'"
     ]
    }
   ],
   "source": [
    "bucket='sn2020-final-project' # Replace with your s3 bucket name\n",
    "file = 'DenseNet MLT.ipynp'\n",
    "\n",
    "\n",
    "url = 's3://{}/{}'.format(bucket, file)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(file).upload_file(file)\n",
    "print('Done writing to {}'.format(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets plot the results\n",
    "print(history.history.keys())\n",
    "# history accuracy\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history accuracy\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.Session().resource('s3').Bucket(bucket).Object(keyweights).download_file('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EtSuaNhNRAz1"
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model = createModel()\n",
    "model.load_weights(\"weights.h5\")\n",
    "\n",
    "img_path = test_data + os.sep + \"gorilla\" + os.sep + \"18.jpeg\"\n",
    "img = load_img(img_path, target_size=(n_H, n_W))\n",
    "\n",
    "imgArray = img_to_array(img)\n",
    "# Update to 0-1 format\n",
    "imgArray = imgArray / 255.0\n",
    "print(imgArray.shape)\n",
    "plt.imshow(imgArray)\n",
    "# The modelexspect batches then lets do a batch of 1 \n",
    "imgArray = np.expand_dims(imgArray, axis=0)\n",
    "\n",
    "# Predict\n",
    "classList = os.listdir(test_data)\n",
    "prediction = model.predict(imgArray)\n",
    "print(prediction)\n",
    "prediction = model.predict_classes(imgArray)\n",
    "print(classList[prediction[0]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict the gorillas\n",
    "gorillapath = test_data + os.sep + \"cat\" + os.sep\n",
    "imagesPredict = [f for f in listdir(gorillapath) if isfile(join(gorillapath,f))]\n",
    "# print(imagesPredict)\n",
    "\n",
    "for file in imagesPredict:\n",
    "    img = load_img(gorillapath + file, target_size=( n_W, n_H))\n",
    "    tensorImage = img_to_array(img) / 255.\n",
    "    tensorImage = np.expand_dims(tensorImage, axis=0)\n",
    "    gorillaPrediction = model.predict_classes(tensorImage, batch_size = 1)\n",
    "    print(\"file \"+ file + \" is: \" + classList[gorillaPrediction[0]])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ex07_training.ipynb",
   "provenance": [
    {
     "file_id": "1-SzGPs1Bo7BnPiwhMH66D2uy_9NYXvgD",
     "timestamp": 1570577817835
    }
   ]
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
